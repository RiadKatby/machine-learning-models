{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01dd6a9",
   "metadata": {},
   "source": [
    "# Build Chatbot NLU\n",
    "## 1. Load CSV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5f22a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "Questions_PATH=\"./\"\n",
    "\n",
    "def load_dataset(dataset_path=Questions_PATH):\n",
    "    csv_path = os.path.join(dataset_path, \"question-intent.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "dataset = load_dataset()\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d85298",
   "metadata": {},
   "source": [
    "## 2. Preprocess Dataset - Input\n",
    "2.1 Seplit Questions into Words\n",
    "\n",
    "2.2 Remove Stopwords\n",
    "\n",
    "2.3 Root words\n",
    "\n",
    "2.4 Re-join word in one statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef6a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "dataset['جذور السؤال'] = ''\n",
    "\n",
    "st = ISRIStemmer()\n",
    "_stopwords = set(stopwords.words('arabic') + list(punctuation))\n",
    "\n",
    "def preprocess(text):\n",
    "    words = word_tokenize(str(text))\n",
    "    words = [word for word in words if word not in _stopwords]\n",
    "    words = [st.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    dataset['جذور السؤال'][index] = preprocess(row['السؤال'])\n",
    "    \n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b29608",
   "metadata": {},
   "source": [
    "## 3. Preprocess Dataset - Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25ed941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "label_cat = dataset[['الخدمة']]\n",
    "\n",
    "cat_encoder = OneHotEncoder()\n",
    "label_cat_1hot = cat_encoder.fit_transform(label_cat).toarray()\n",
    "#label_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ab00f",
   "metadata": {},
   "source": [
    "## 4. Shuffle and Split into train, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ffc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "sentences = dataset['جذور السؤال'].to_numpy()\n",
    "X, Y = unison_shuffled_copies(sentences, label_cat_1hot)\n",
    "\n",
    "training_size = int(len(X) * 0.8)\n",
    "train_X, test_X = X[:training_size], X[training_size:]\n",
    "train_Y, test_Y = Y[:training_size], Y[training_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_Y))\n",
    "\n",
    "print(len(test_X))\n",
    "print(len(test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3772bd0",
   "metadata": {},
   "source": [
    "## 5. Convert to Sequence, and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f48410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "oov_tok = \"<OOV>\"\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "max_length = 100\n",
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_X_seq = tokenizer.texts_to_sequences(train_X)\n",
    "train_X_pad = pad_sequences(train_X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "test_X_seq = tokenizer.texts_to_sequences(test_X)\n",
    "test_X_pad = pad_sequences(test_X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d3de7",
   "metadata": {},
   "source": [
    "## 6. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a072024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "lstm_dim = 32\n",
    "dense_dim = 16\n",
    "\n",
    "opt = Adamax(learning_rate=0.03, beta_1=0.8, beta_2=0.9999)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim)),\n",
    "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6f11a",
   "metadata": {},
   "source": [
    "## 7. Train, and Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "history = model.fit(train_X_pad, train_Y, epochs=num_epochs, validation_data=(test_X_pad, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6d623",
   "metadata": {},
   "source": [
    "## 8. Plot Accuracy and Loss / Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933287af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211e265",
   "metadata": {},
   "source": [
    "## 9. Test and Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8e9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = preprocess('قم بتجديد رخصتي التجارية')\n",
    "train_X_seq = tokenizer.texts_to_sequences([sen])\n",
    "train_X_pad = pad_sequences(train_X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "result = model.predict(train_X_pad)\n",
    "print(cat_encoder.categories_[0][result.argmax()])\n",
    "print(result.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = result.argsort()\n",
    "predicted_result = nr[0][-4:]\n",
    "for i in predicted_result[::-1]:\n",
    "    print(result[0][i])\n",
    "    print(cat_encoder.categories_[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927a2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
